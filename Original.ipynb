{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.attack_algo_utils import *\n",
    "from utils.graph_utils import *\n",
    "from utils.utils import *"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- CHỌN PHIÊN BẢN THÍ NGHIỆM ---\n",
    "experiment_id = 1\n",
    "# BASE_PATH = f'graphs/{experiment_id}'\n",
    "# BASE_PATH  =  f'graph_test'\n",
    "BASE_PATH  =  f'graph_test_delete'\n"
   ],
   "id": "f764200a08e5f484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- LOAD ENVIRONMENTAL INFORMATION ---\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Tải Môi trường (Tĩnh) ---\n",
    "STATIC_FILE_PATH = f\"{BASE_PATH}/graph_environment.pth\"\n",
    "\n",
    "try:\n",
    "    env_data = torch.load(STATIC_FILE_PATH, weights_only=False)\n",
    "\n",
    "    G = env_data['G_original']\n",
    "    node_order = env_data['node_order']\n",
    "    node_map = env_data['node_map']\n",
    "\n",
    "    # Bạn cũng có thể lấy features gốc nếu cần\n",
    "    # node_features_goc = env_data['node_features_original']\n",
    "\n",
    "    print(f\"--- Đã tải môi trường tĩnh từ '{STATIC_FILE_PATH}' ---\")\n",
    "    print(\"Tổng số node:\", len(node_order))\n",
    "    print(\"Map của 'Host 1':\", node_map['Host 1'])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"LỖI: Không tìm thấy tệp '{STATIC_FILE_PATH}'.\")\n",
    "    print(\"Vui lòng kiểm tra lại experiment_id hoặc đường dẫn.\")\n",
    "    # Thoát hoặc xử lý lỗi nếu cần\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 2. Tải Embeddings (Động) ---\n",
    "NODE_EMB_PATH = f\"{BASE_PATH}/node_embeddings.npy\"\n",
    "EDGE_EMB_PATH = f\"{BASE_PATH}/edge_embeddings.npy\"\n",
    "\n",
    "try:\n",
    "    nodes_emb = np.load(NODE_EMB_PATH)\n",
    "    edges_emb = np.load(EDGE_EMB_PATH)\n",
    "\n",
    "    print(f\"\\n--- Đã tải embedding động từ thí nghiệm {experiment_id} ---\")\n",
    "    print(\"Shape của Node Embeddings:\", nodes_emb.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"LỖI: Không tìm thấy tệp '{NODE_EMB_PATH}' hoặc '{EDGE_EMB_PATH}'.\")\n",
    "    # Thoát hoặc xử lý lỗi nếu cần\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 3. Sử dụng ---\n",
    "# Giờ đây bạn đã có cả hai:\n",
    "# - `node_map` để biết \"Host 1\" là ID số mấy.\n",
    "# - `nodes_emb` để lấy embedding của ID đó.\n",
    "\n",
    "try:\n",
    "    node_name = \"Host 1\"\n",
    "    node_id = node_map[node_name]\n",
    "    embedding_cua_host_1 = nodes_emb[node_id]\n",
    "\n",
    "    print(f\"\\n--- Sẵn sàng cho RL ---\")\n",
    "    print(f\"Embedding cho '{node_name}' (ID: {node_id}): \\n\", embedding_cua_host_1)\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"Lỗi: Không tìm thấy node '{node_name}' trong node_map.\")\n",
    "except IndexError:\n",
    "    print(f\"Lỗi: node_id {node_id} nằm ngoài phạm vi của 'nodes_emb' (Shape: {nodes_emb.shape})\")"
   ],
   "id": "8626fc0ee2bc265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Visualize the graph ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10)\n",
    "edge_labels = nx.get_edge_attributes(G, 'user')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "plt.title(\"Attack Graph (Edge Weights: User Access Prob)\")\n",
    "plt.show()"
   ],
   "id": "951c0a778822db3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(nodes_emb)",
   "id": "a60ffc5d033479e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(edges_emb)",
   "id": "1021c57f0d9d6566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ======================================================================\n",
    "# TẢI DỮ LIỆU MÔI TRƯỜNG TĨNH\n",
    "# ======================================================================\n",
    "print(\"--- Đang tải dữ liệu môi trường (tĩnh) ---\")\n",
    "# Giả định BASE_PATH đã được định nghĩa\n",
    "# Giả định các class (DGI, EGraphSAGE, TGNWrapperEncoder...) đã được định nghĩa\n",
    "# Giả định các thư viện (torch, F, dgl) đã được import\n",
    "\n",
    "env_data = torch.load(f\"{BASE_PATH}/graph_environment.pth\",weights_only=False)\n",
    "\n",
    "g_dgl = env_data.get('g_dgl')\n",
    "if g_dgl is None:\n",
    "    G_original = env_data['G_original']\n",
    "    g_dgl = dgl.from_networkx(G_original, node_attrs=['state', 'priority'], edge_attrs=['user', 'root'])\n",
    "    g_dgl.ndata['h'] = env_data['node_features_original']\n",
    "    g_dgl.edata['h'] = env_data['edge_features_original']\n",
    "    print(\"Đã tạo lại g_dgl từ G_original.\")\n",
    "\n",
    "original_edge_features = env_data['edge_features_original']\n",
    "original_node_features = env_data['node_features_original']\n",
    "static_priority_features = original_node_features[:, 1].unsqueeze(1) # Cột priority\n",
    "\n",
    "print(\"[THÀNH CÔNG] Đã tải xong dữ liệu môi trường.\")\n",
    "\n",
    "# ======================================================================\n",
    "# TẢI MODEL GNN ĐÃ HUẤN LUYỆN (ĐÃ SỬA LỖI)\n",
    "# ======================================================================\n",
    "print(\"--- Đang tải cấu hình và trọng số GNN ---\")\n",
    "\n",
    "MODEL_STATE_PATH = f\"{BASE_PATH}/dgi_model_state_dict.pth\"\n",
    "CONFIG_FILE_PATH = f\"{BASE_PATH}/model_config.pth\"\n",
    "\n",
    "try:\n",
    "    # --- 3.1: Tải file cấu hình ---\n",
    "    config = torch.load(CONFIG_FILE_PATH, weights_only=False)\n",
    "    print(f\"Đã tải cấu hình: {config}\")\n",
    "\n",
    "    # --- 3.2: Khởi tạo mô hình rỗng (ĐÃ SỬA) ---\n",
    "    \n",
    "    # Lấy số nút gốc từ đồ thị đã tải\n",
    "    NUM_NODES_ORIGINAL = g_dgl.num_nodes() # Đây sẽ là 9\n",
    "    \n",
    "    # Giả định các tham số TGN này khớp với lúc huấn luyện\n",
    "    # (Vì chúng không có trong file config)\n",
    "    MEMORY_DIM = 32 \n",
    "    MSG_DIM = 32\n",
    "    \n",
    "    print(f\"Khởi tạo TGNWrapperEncoder với {NUM_NODES_ORIGINAL} nút (gốc)...\")\n",
    "    \n",
    "    # Khởi tạo đúng TGNWrapperEncoder\n",
    "    encoder = TGNWrapperEncoder(\n",
    "        num_nodes=NUM_NODES_ORIGINAL, # 9 nút\n",
    "        node_feat_dim=config['NDIM_IN'],\n",
    "        edge_feat_dim=config['EDIM'],\n",
    "        memory_dim=MEMORY_DIM,\n",
    "        msg_dim=MSG_DIM,\n",
    "        sage_n_hidden=config['N_HIDDEN'],\n",
    "        sage_n_out=config['N_OUT'],\n",
    "        sage_n_layers=config['N_LAYERS'],\n",
    "        sage_activation=F.leaky_relu\n",
    "    )\n",
    "\n",
    "    # Khởi tạo DGI với TGNWrapperEncoder\n",
    "    dgi_model_to_load = DGI(encoder) # Model này có memory [9, 32]\n",
    "# --- 3.3: Tải trọng số đã lưu (SỬA CƠ CHẾ TẢI) ---\n",
    "    print(\"Đang tải state_dict (với cơ chế xử lý mismatch)...\")\n",
    "    \n",
    "    # Tải checkpoint (chùm chìa khóa) vào một biến riêng\n",
    "    checkpoint_state = torch.load(MODEL_STATE_PATH, weights_only=False)\n",
    "    \n",
    "    # Tách 'encoder.memory' ra khỏi checkpoint\n",
    "    memory_from_checkpoint = checkpoint_state.pop(\"encoder.memory\", None)\n",
    "    \n",
    "    if memory_from_checkpoint is None:\n",
    "        raise KeyError(\"Không tìm thấy 'encoder.memory' trong checkpoint. File model có thể bị hỏng.\")\n",
    "        \n",
    "    # Lấy số nút từ checkpoint (ví dụ: 8)\n",
    "    num_nodes_checkpoint = memory_from_checkpoint.shape[0]\n",
    "    \n",
    "    # BƯỚC 1: Luôn tải tất cả các trọng số KHÁC (GRU, Linear, Sage...)\n",
    "    # Dùng strict=False vì checkpoint_state BỊ THIẾU 'encoder.memory'\n",
    "    # và cũng có thể bị thiếu các hàm 'delete' (edge_del_msg_fn...)\n",
    "    # mà model MỚI (dgi_model_to_load) đang có.\n",
    "    print(\"Đang tải các trọng số chung (GRU, SAGE, v.v.)...\")\n",
    "    dgi_model_to_load.load_state_dict(checkpoint_state, strict=False)\n",
    "    print(\"Đã tải các trọng số chung thành công.\")\n",
    "\n",
    "    # BƯỚC 2: Xử lý sao chép memory thủ công\n",
    "    \n",
    "    # Lấy số nút của model HIỆN TẠI (ví dụ: 9)\n",
    "    num_nodes_model = dgi_model_to_load.encoder.num_nodes\n",
    "    \n",
    "    # Xác định số lượng nút tối thiểu để sao chép\n",
    "    num_to_copy = min(num_nodes_checkpoint, num_nodes_model)\n",
    "    \n",
    "    print(f\"Đang sao chép memory: Checkpoint ({num_nodes_checkpoint} nút) -> Model ({num_nodes_model} nút).\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Lấy tensor memory của model hiện tại\n",
    "        current_model_memory = dgi_model_to_load.encoder.memory.data\n",
    "        \n",
    "        # Lấy phần memory tương ứng từ checkpoint\n",
    "        checkpoint_memory_slice = memory_from_checkpoint[:num_to_copy]\n",
    "        \n",
    "        # Ghi đè phần chung\n",
    "        # - Nếu model=9, checkpoint=8: Ghi đè 8 nút đầu tiên của model\n",
    "        # - Nếu model=7, checkpoint=8: Ghi đè 7 nút đầu tiên của model\n",
    "        current_model_memory[:num_to_copy] = checkpoint_memory_slice\n",
    "    \n",
    "    print(f\"Đã sao chép thủ công memory của {num_to_copy} nút chung.\")\n",
    "    \n",
    "    # In thông báo trạng thái\n",
    "    if num_nodes_model > num_nodes_checkpoint:\n",
    "        # Ví dụ: Model=9, Checkpoint=8\n",
    "        print(f\"  -> Model có {num_nodes_model - num_nodes_checkpoint} nút mới. Các nút này sẽ giữ memory khởi tạo (zeros).\")\n",
    "    elif num_nodes_checkpoint > num_nodes_model:\n",
    "        # Ví dụ: Model=7, Checkpoint=8\n",
    "        print(f\"  -> Model nhỏ hơn checkpoint. Đã bỏ qua {num_nodes_checkpoint - num_nodes_model} nút từ checkpoint.\")\n",
    "    else:\n",
    "        # Ví dụ: Model=8, Checkpoint=8\n",
    "        print(\"  -> Kích thước memory khớp hoàn toàn.\")\n",
    "\n",
    "    print(\"Tải state_dict thành công.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[LỖI] Có lỗi xảy ra khi tải model: {e}\")\n",
    "    trained_encoder = None\n",
    "\n"
   ],
   "id": "cdd8d9eedf83bf44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def train_dqn(env, num_episodes, batch_size=10, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "#     global best_checkpoint, best_episode\n",
    "#     state_size = env.num_nodes\n",
    "#     action_space_size = env.get_action_space_size()\n",
    "#     print('state_size', state_size)\n",
    "#     print('action_space_size', action_space_size)\n",
    "#     # Initialize DQN and target network\n",
    "#     policy_net = DQN(state_size, action_space_size)\n",
    "#     target_net = DQN(state_size, action_space_size)\n",
    "#     target_net.load_state_dict(policy_net.state_dict())\n",
    "#     target_net.eval()\n",
    "#\n",
    "#     optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "#     replay_buffer = ReplayBuffer(capacity=10000)\n",
    "#     epsilon = epsilon_start\n",
    "#     total_reward = 0\n",
    "#     dsp = 0\n",
    "#     best_dsp = 0\n",
    "#     interval_check = num_episodes // 10  # Mỗi num_episodes/10\n",
    "#     interval_save = num_episodes // 5   # Lưu sau mỗi num_episodes/5\n",
    "#\n",
    "#     for episode in range(1, num_episodes+1):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#\n",
    "#         exploration_counter = defaultdict(int)\n",
    "#\n",
    "#         while not done:\n",
    "#             if random.random() < epsilon:\n",
    "#                 # Chọn ngẫu nhiên index hợp lệ\n",
    "#                 action_idx = sample_valid_index(action_space_size, env.num_honeypot_nodes, exploration_counter)\n",
    "#             else:\n",
    "#                 with torch.no_grad():\n",
    "#                     state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "#                     q_values = policy_net(state_tensor).squeeze(0)  # shape: [action_space_size]\n",
    "#\n",
    "#                     # Lọc q_values chỉ lấy index hợp lệ\n",
    "#                     valid_indices = [idx for idx in range(action_space_size) if is_valid_index(idx, env.num_honeypot_nodes)]\n",
    "#                     valid_q_values = q_values[valid_indices]\n",
    "#                     # Lấy chỉ số trong valid_indices có q_value max\n",
    "#                     max_idx_in_valid = torch.argmax(valid_q_values).item()\n",
    "#                     # Map về action_idx thực\n",
    "#                     action_idx = valid_indices[max_idx_in_valid]\n",
    "#\n",
    "#             action = index_to_action(action_idx, env.num_honeypot_nodes)\n",
    "#             next_state, reward, done, path, captured = env.step(action)\n",
    "#             action_idx = action_to_index(action, env.num_honeypot_nodes)\n",
    "#\n",
    "#             # Store experience\n",
    "#             replay_buffer.push(state, action_idx, reward, next_state, done)\n",
    "#             state = next_state\n",
    "#             total_reward += reward\n",
    "#             if reward == 1:\n",
    "#                 dsp += 1\n",
    "#             # Train if enough experiences\n",
    "#             if len(replay_buffer) >= batch_size:\n",
    "#                 states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#\n",
    "#                 states = torch.FloatTensor(states)\n",
    "#                 actions = torch.LongTensor(actions)\n",
    "#                 rewards = torch.FloatTensor(rewards)\n",
    "#                 next_states = torch.FloatTensor(next_states)\n",
    "#                 dones = torch.FloatTensor(dones)\n",
    "#\n",
    "#                 # Compute Q-values\n",
    "#                 q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "#\n",
    "#                 # Compute target Q-values\n",
    "#                 with torch.no_grad():\n",
    "#                     next_q_values = target_net(next_states).max(1)[0]\n",
    "#                     targets = rewards + (1 - dones) * gamma * next_q_values\n",
    "#\n",
    "#                 # Compute loss\n",
    "#                 loss = nn.MSELoss()(q_values, targets)\n",
    "#\n",
    "#                 # Optimize\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#\n",
    "#         # Update target network\n",
    "#         if episode % 10 == 0:\n",
    "#             target_net.load_state_dict(policy_net.state_dict())\n",
    "#\n",
    "#         # Decay epsilon\n",
    "#         epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "#\n",
    "#         # Logging\n",
    "#         if episode % interval_check == 0:\n",
    "#             placement = []\n",
    "#             for i in range(2):  # Two honeypots\n",
    "#                 node_idx = np.argmax(action[i])\n",
    "#                 node_name = env.honeypot_nodes[node_idx]\n",
    "#                 placement.append(f\"Honeypot {i} -> {node_name}\\n\")\n",
    "#             print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}, Defense Success Probability: {dsp/interval_check}%\\n\")\n",
    "#             print(\"\".join(placement))\n",
    "#             print(path)\n",
    "#             total_reward = 0\n",
    "#\n",
    "#             # Log ra DSP lớn nhất sau mỗi num_episodes/10 iterations\n",
    "#             if dsp > best_dsp:\n",
    "#                 best_dsp = dsp\n",
    "#                 best_episode = episode\n",
    "#                 best_checkpoint = {\n",
    "#                     'policy_net_state_dict': deepcopy(policy_net.state_dict()),\n",
    "#                     'target_net_state_dict': deepcopy(target_net.state_dict()),\n",
    "#                     'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
    "#                 }\n",
    "#             # Reset DSP\n",
    "#             dsp = 0\n",
    "#\n",
    "#\n",
    "#         # Save ra DSP lớn nhất sau mỗi num_episodes/5 iterations\n",
    "#         if (episode + 1) % interval_save == 0 and best_checkpoint is not None:\n",
    "#             path = f'./Saved_Model/dqn_model.pth'\n",
    "#             torch.save({\n",
    "#                 'policy_net_state_dict': best_checkpoint['policy_net_state_dict'],\n",
    "#                 'target_net_state_dict': best_checkpoint['target_net_state_dict'],\n",
    "#                 'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],\n",
    "#                 'episode': best_episode},\n",
    "#                 path)\n",
    "#             print(f'Saved model with best DSP {best_dsp} at episode {best_episode} to {path}')\n",
    "#\n",
    "#             best_dsp = 0\n",
    "#             best_episode = 0\n",
    "#             best_checkpoint = None\n",
    "#\n",
    "#     return policy_net"
   ],
   "id": "5377d66c524211df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "def train_dqn(env, num_episodes, batch_size=10, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "    global best_checkpoint, best_episode\n",
    "    # 1. Reset env để lấy state (embedding) ban đầu\n",
    "    state = env.reset() # state giờ là Tensor [num_nodes, embedding_dim]\n",
    "\n",
    "    # 2. Tính toán state_size đã làm phẳng\n",
    "    num_nodes = state.shape[0]\n",
    "    embedding_dim = state.shape[1]\n",
    "    state_size = num_nodes * embedding_dim  # <--- Kích thước input mới cho DQN\n",
    "\n",
    "    action_space_size = env.get_action_space_size()\n",
    "\n",
    "    print('state_size (flattened):', state_size) # <--- Cập nhật log\n",
    "    print('action_space_size', action_space_size)\n",
    "\n",
    "    # 3. Khởi tạo DQN với state_size mới\n",
    "    policy_net = DQN(state_size, action_space_size)\n",
    "    target_net = DQN(state_size, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "    epsilon = epsilon_start\n",
    "    total_reward = 0\n",
    "    dsp = 0\n",
    "    best_dsp = 0\n",
    "    interval_check = num_episodes // 10\n",
    "    interval_save = num_episodes // 5\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Reset state cho các episode sau\n",
    "        if episode > 1:\n",
    "            state = env.reset() # <--- state là Tensor embedding\n",
    "\n",
    "        done = False\n",
    "        exploration_counter = defaultdict(int)\n",
    "\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action_idx = sample_valid_index(action_space_size, env.num_honeypot_nodes, exploration_counter)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    # --- THAY ĐỔI: Flatten state tensor ---\n",
    "                    # Chuyển [N, D] -> [1, N*D]\n",
    "                    state_tensor = state.flatten().unsqueeze(0)\n",
    "                    q_values = policy_net(state_tensor).squeeze(0)\n",
    "\n",
    "                    # (Logic lọc q_values giữ nguyên)\n",
    "                    valid_indices = [idx for idx in range(action_space_size) if is_valid_index(idx, env.num_honeypot_nodes)]\n",
    "                    valid_q_values = q_values[valid_indices]\n",
    "                    max_idx_in_valid = torch.argmax(valid_q_values).item()\n",
    "                    action_idx = valid_indices[max_idx_in_valid]\n",
    "\n",
    "            action = index_to_action(action_idx, env.num_honeypot_nodes)\n",
    "\n",
    "            # --- next_state giờ cũng là Tensor embedding ---\n",
    "            next_state, reward, done, path, captured = env.step(action)\n",
    "            action_idx = action_to_index(action, env.num_honeypot_nodes)\n",
    "\n",
    "            # Store experience (state và next_state là Tensors)\n",
    "            replay_buffer.push(state, action_idx, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if reward == 1:\n",
    "                dsp += 1\n",
    "\n",
    "            # Train if enough experiences\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # --- THAY ĐỔI: Replay buffer giờ trả về Tensors ---\n",
    "                states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # states_batch là [B, N, D], actions_batch là [B], rewards_batch là [B, 1], ...\n",
    "\n",
    "                # --- THAY ĐỔI: Flatten state batches ---\n",
    "                # Chuyển [B, N, D] -> [B, N*D]\n",
    "                states_flat = states_batch.flatten(start_dim=1)\n",
    "                next_states_flat = next_states_batch.flatten(start_dim=1)\n",
    "\n",
    "                # Compute Q-values\n",
    "                q_values_all = policy_net(states_flat)\n",
    "                # Dùng actions_batch để lấy Q-value của action đã chọn\n",
    "                q_values = q_values_all.gather(1, actions_batch.long().unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    # Dùng next_states_flat\n",
    "                    next_q_values = target_net(next_states_flat).max(1)[0]\n",
    "                    # Squeeze rewards và dones để khớp kích thước [B]\n",
    "                    targets = rewards_batch.squeeze(1) + (1 - dones_batch.squeeze(1)) * gamma * next_q_values\n",
    "\n",
    "                # Compute loss\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # Logging (Giữ nguyên)\n",
    "        if episode % interval_check == 0:\n",
    "            placement = []\n",
    "            for i in range(2):\n",
    "                node_idx = np.argmax(action[i])\n",
    "                node_name = env.honeypot_nodes[node_idx]\n",
    "                placement.append(f\"Honeypot {i} -> {node_name}\\n\")\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}, Defense Success Probability: {dsp/interval_check}%\\n\")\n",
    "            print(\"\".join(placement))\n",
    "            print(path)\n",
    "            total_reward = 0\n",
    "\n",
    "            if dsp > best_dsp:\n",
    "                best_dsp = dsp\n",
    "                best_episode = episode\n",
    "                best_checkpoint = {\n",
    "                    'policy_net_state_dict': deepcopy(policy_net.state_dict()),\n",
    "                    'target_net_state_dict': deepcopy(target_net.state_dict()),\n",
    "                    'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
    "                }\n",
    "            dsp = 0\n",
    "        \n",
    "        # Save (Giữ nguyên)\n",
    "        if (episode + 1) % interval_save == 0 and best_checkpoint is not None:\n",
    "            path = f'./saved_model/dqn_model.pth'\n",
    "            torch.save({\n",
    "                'policy_net_state_dict': best_checkpoint['policy_net_state_dict'],\n",
    "                'target_net_state_dict': best_checkpoint['target_net_state_dict'],\n",
    "                'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],\n",
    "                'episode': best_episode},\n",
    "                path)\n",
    "            print(f'Saved model with best DSP {best_dsp} at episode {best_episode} to {path}')\n",
    "\n",
    "            best_dsp = 0\n",
    "            best_episode = 0\n",
    "            best_checkpoint = None\n",
    "\n",
    "    return policy_net"
   ],
   "id": "1c293e1d3a7ce59a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(30, 36))\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color='orange', node_size=2000)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    G, pos,\n",
    "    edge_color='gray',\n",
    "    arrows=True,\n",
    "    arrowstyle='->',\n",
    "    arrowsize=50,\n",
    "    connectionstyle='arc3,rad=0.2'\n",
    ")\n",
    "\n",
    "# Vẽ nhãn trên cạnh\n",
    "edge_labels = {(u, v): f\"user={d['user']}, root={d['root']}\" for u, v, d in G.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "4b79226cb400a676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize environment and train\n",
    "algo = global_weighted_random_attack\n",
    "# algo = greedy_attack_priority_queue\n",
    "# Tạo một bản sao của đồ thị cho môi trường\n",
    "G_new_env = deepcopy(G)\n",
    "\n",
    "env = NetworkEnv(\n",
    "    G_new=G_new_env,\n",
    "    attack_fn=algo,\n",
    "    g_dgl=g_dgl,\n",
    "    encoder=encoder,\n",
    "    original_node_features=original_node_features,\n",
    "    original_edge_features=original_edge_features,\n",
    "    node_map=node_map,\n",
    "    goal=\"Data Server\"  # (Hoặc goal bạn muốn)\n",
    ")\n",
    "!mkdir saved_model\n",
    "# --- 3. HUẤN LUYỆN (Như cũ) ---\n",
    "num_episode = 2000\n",
    "model = train_dqn(env, num_episode)"
   ],
   "id": "3b20203b7880a2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_model(model,env)   ",
   "id": "597785563e2df568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "df0a9656985faee2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
