{
 "cells": [
  {
   "cell_type": "code",
   "id": "5eaf6dee",
   "metadata": {},
   "source": [
    "# --- CUDA DEVICE SETUP ---\n",
    "import torch\n",
    "\n",
    "# Tự động chọn GPU nếu có\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Khi cần chuyển dữ liệu: \n",
    "# tensor = tensor.to(device)\n",
    "# model = model.to(device)\n",
    "# g = g.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md_imports",
   "metadata": {},
   "source": [
    "## Sổ tay Tác nhân RL (Đã sửa lỗi)\n",
    "\n",
    "Sổ tay này tải môi trường, cấu hình GNN, và trọng số GNN đã huấn luyện (từ `Graph_Canonical_50.ipynb`) để chạy Tác nhân DQN."
   ]
  },
  {
   "cell_type": "code",
   "id": "imports_cell",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import dgl\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import random\n",
    "# Giả định các tệp utils này tồn tại trong thư mục của bạn\n",
    "from utils.attack_algo_utils import *\n",
    "from utils.graph_utils import *\n",
    "from utils.utils import *\n",
    "\n",
    "print(\"--- CHUẨN BỊ MÔI TRƯỜNG ---\")\n",
    "\n",
    "# --- CHỌN PHIÊN BẢN THÍ NGHIỆM ---\n",
    "experiment_id = 1\n",
    "BASE_PATH = f'graphs/{experiment_id}'\n",
    "print(f\"Đang chạy thí nghiệm ID: {experiment_id} tại đường dẫn: {BASE_PATH}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "load_static_env",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# 1. TẢI DỮ LIỆU MÔI TRƯỜNG TĨNH\n",
    "# ======================================================================\n",
    "print(\"\\n--- Đang tải dữ liệu môi trường (tĩnh) ---\")\n",
    "STATIC_FILE_PATH = f\"{BASE_PATH}/graph_environment.pth\"\n",
    "NODE_EMB_PATH = f\"{BASE_PATH}/node_embeddings.npy\"\n",
    "\n",
    "try:\n",
    "    env_data = torch.load(STATIC_FILE_PATH, weights_only=False)\n",
    "    \n",
    "    G_original = env_data['G_original']\n",
    "    node_order = env_data['node_order']\n",
    "    node_map = env_data['node_map']\n",
    "    \n",
    "    # Lấy các đặc trưng GỐC (ví dụ: [8, 2])\n",
    "    original_node_features = env_data['node_features_original']\n",
    "    original_edge_features = env_data['edge_features_original']\n",
    "    g1 = env_data['g1'] # Đồ thị DGL gốc\n",
    "    \n",
    "    # Lấy cột priority (ví dụ: [8, 1])\n",
    "    static_priority_features = original_node_features[:, 1].unsqueeze(1) \n",
    "\n",
    "    nodes_emb = np.load(NODE_EMB_PATH) # Tải embedding đã huấn luyện\n",
    "\n",
    "    print(f\"Đã tải môi trường tĩnh từ '{STATIC_FILE_PATH}'\")\n",
    "    print(f\"Tổng số node: {len(node_order)}\")\n",
    "    print(f\"Shape của Node Embeddings (đã huấn luyện): {nodes_emb.shape}\")\n",
    "    print(f\"Shape của Đặc trưng Node Gốc: {original_node_features.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"LỖI: Không tìm thấy hoặc không đọc được tệp môi trường/embedding: {e}\")\n",
    "    print(\"Vui lòng chạy sổ tay 'Graph.ipynb' (huấn luyện) trước.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "define_preprocessing",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# 2. ĐỊNH NGHĨA HÀM TIỀN XỬ LÝ (PHẢI GIỐNG HỆT FILE HUẤN LUYỆN)\n",
    "# ======================================================================\n",
    "# (Chúng ta cần các hằng số và hàm này để xử lý\n",
    "#  đặc trưng [8, 2] thành [8, 50] trước khi đưa vào encoder)\n",
    "\n",
    "MAX_N_FEATURES = 2\n",
    "MAX_E_FEATURES = 2\n",
    "\n",
    "def build_batch_tensor(feats, max_dim):\n",
    "    return feats[:, :max_dim]\n",
    "\n",
    "print(\"Đã định nghĩa hàm tiền xử lý (padding + normalization).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "load_gnn_model",
   "metadata": {},
   "source": [
    "# ======================================================================\n",
    "# 3. TẢI MODEL GNN ĐÃ HUẤN LUYỆN (PHẦN ĐÃ SỬA)\n",
    "# ======================================================================\n",
    "print(\"\\n--- Đang tải cấu hình và trọng số GNN ---\")\n",
    "\n",
    "MODEL_STATE_PATH = f\"graphs/dgi_model_state_dict.pth\"\n",
    "CONFIG_FILE_PATH = f\"{BASE_PATH}/model_config.pth\"\n",
    "\n",
    "try:\n",
    "    # --- 3.1: Tải file cấu hình ---\n",
    "    config = torch.load(CONFIG_FILE_PATH, weights_only=False)\n",
    "    print(f\"Đã tải cấu hình: {config}\")\n",
    "\n",
    "    # --- 3.2: Khởi tạo mô hình rỗng TỪ CẤU HÌNH ĐÃ TẢI ---\n",
    "    encoder = EGraphSAGE(\n",
    "        config['NDIM_IN'],       # 50\n",
    "        config['EDIM'],          # 50\n",
    "        config['N_HIDDEN'],\n",
    "        config['N_OUT'],\n",
    "        config['N_LAYERS'],\n",
    "        F.leaky_relu,\n",
    "    )\n",
    "\n",
    "    dgi_model_to_load = DGI(encoder)\n",
    "\n",
    "    # --- 3.3: Tải trọng số đã lưu ---\n",
    "    dgi_model_to_load.load_state_dict(torch.load(MODEL_STATE_PATH, weights_only=False))\n",
    "\n",
    "    # --- 3.4: Trích xuất encoder bạn cần ---\n",
    "    trained_encoder = dgi_model_to_load.encoder\n",
    "    trained_encoder.to(device)\n",
    "    trained_encoder.eval() # Chuyển sang chế độ dự đoán\n",
    "\n",
    "    print(f\"[THÀNH CÔNG] Đã tải và trích xuất GNN encoder.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[LỖI] Có lỗi xảy ra khi tải model: {e}\")\n",
    "    trained_encoder = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "define_dqn_logic",
   "metadata": {},
   "source": [
    "def train_dqn(env, num_episodes, batch_size=10, gamma=0.99, \n",
    "              epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, device=None):\n",
    "    \n",
    "    global best_checkpoint, best_episode\n",
    "\n",
    "    # --- Chọn device ---\n",
    "    device = device or (torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # --- Reset env lấy state ban đầu và chuyển sang device ---\n",
    "    state = env.reset().to(device) # Shape [M, D_embed]\n",
    "\n",
    "    num_nodes_M = state.shape[0]\n",
    "    embedding_dim = state.shape[1]\n",
    "    num_honeypots_N = env.num_honeypots\n",
    "    \n",
    "    state_size = num_nodes_M * embedding_dim\n",
    "\n",
    "    print(f'State size (flattened): {state_size}')\n",
    "    print(f'Honeypots (N): {num_honeypots_N}, Nodes (M): {num_nodes_M}')\n",
    "    print(f'Action output shape: ({num_honeypots_N}, {num_nodes_M})')\n",
    "\n",
    "    # --- Khởi tạo mô hình (MULTI-HEAD) trên device ---\n",
    "    policy_net = MultiHeadDQN(state_size, num_honeypots_N, num_nodes_M).to(device)\n",
    "    target_net = MultiHeadDQN(state_size, num_honeypots_N, num_nodes_M).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "    epsilon = epsilon_start\n",
    "    total_reward = 0\n",
    "    dsp = 0\n",
    "    best_dsp = 0\n",
    "    interval_check = max(1, num_episodes // 10)\n",
    "    interval_save = max(1, num_episodes // 5)\n",
    "    best_checkpoint = None\n",
    "    best_episode = 0\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        if episode > 1:\n",
    "            state = env.reset().to(device)\n",
    "        \n",
    "        done = False\n",
    "        # exploration_counter không còn cần thiết nữa\n",
    "\n",
    "        while not done:\n",
    "            # --- Chọn action ---\n",
    "            if random.random() < epsilon:\n",
    "                # Hành động ngẫu nhiên: Chọn N nút khác nhau từ M\n",
    "                action = sample_valid_action_matrix(num_honeypots_N, num_nodes_M)\n",
    "            else:\n",
    "                # Hành động theo chính sách\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = state.flatten().unsqueeze(0).to(device)\n",
    "                    # q_values shape: [1, N, M]\n",
    "                    q_values = policy_net(state_tensor)\n",
    "                    # squeeze(0) -> [N, M]\n",
    "                    action = select_action_multi_head(q_values.squeeze(0))\n",
    "\n",
    "            # action bây giờ là ma trận [N, M] one-hot (từ numpy)\n",
    "            \n",
    "            # --- Lấy next_state và chuyển sang device ---\n",
    "            next_state, reward, done, path, captured = env.step(action)\n",
    "            next_state = next_state.to(device)\n",
    "\n",
    "            # --- Push vào replay buffer ---\n",
    "            # Lưu state (tensor), action (numpy matrix), ...\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if reward == 1:\n",
    "                dsp += 1\n",
    "\n",
    "            # --- Huấn luyện khi đủ batch ---\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Chuyển batch sang device\n",
    "                states_batch = states_batch.to(device)\n",
    "                next_states_batch = next_states_batch.to(device)\n",
    "                actions_batch = actions_batch.to(device) # Shape: [B, N, M]\n",
    "                rewards_batch = rewards_batch.to(device)\n",
    "                dones_batch = dones_batch.to(device)\n",
    "\n",
    "                # Flatten batch [B, M, D] -> [B, M*D]\n",
    "                states_flat = states_batch.flatten(start_dim=1)\n",
    "                next_states_flat = next_states_batch.flatten(start_dim=1)\n",
    "\n",
    "                # --- TÍNH TOÁN LOSS (MULTI-HEAD) ---\n",
    "\n",
    "                # 1. Tính Q-values hiện tại (Current Q)\n",
    "                # q_values_all shape: [B, N, M]\n",
    "                q_values_all = policy_net(states_flat)\n",
    "                \n",
    "                # actions_batch là ma trận one-hot [B, N, M]\n",
    "                # Nhân element-wise để lọc ra Q-value của hành động đã chọn\n",
    "                current_q_per_head = q_values_all * actions_batch\n",
    "                \n",
    "                # Tính tổng Q-value của N hành động đã chọn\n",
    "                # sum(dim=2) -> [B, N], sum(dim=1) -> [B]\n",
    "                current_q_total = current_q_per_head.sum(dim=2).sum(dim=1)\n",
    "\n",
    "                # 2. Tính Q-values mục tiêu (Target Q)\n",
    "                with torch.no_grad():\n",
    "                    # next_q_values_all shape: [B, N, M]\n",
    "                    next_q_values_all = target_net(next_states_flat)\n",
    "                    \n",
    "                    # Independent Q-Learners (IQL):\n",
    "                    # Lấy max Q-value cho MỖI ĐẦU (head)\n",
    "                    # .max(dim=2)[0] -> [B, N]\n",
    "                    next_q_values_per_head = next_q_values_all.max(dim=2)[0]\n",
    "                    \n",
    "                    # Tính tổng Q-value lớn nhất của N đầu\n",
    "                    # .sum(dim=1) -> [B]\n",
    "                    next_q_total = next_q_values_per_head.sum(dim=1)\n",
    "\n",
    "                    # Tính target: R + gamma * V(s')\n",
    "                    targets = rewards_batch.squeeze(1) + (1 - dones_batch.squeeze(1)) * gamma * next_q_total\n",
    "\n",
    "                # 3. Tính Loss\n",
    "                loss = nn.MSELoss()(current_q_total, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # --- Cập nhật target network ---\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # --- Decay epsilon ---\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # --- Logging ---\n",
    "        if episode % interval_check == 0:\n",
    "            placement = []\n",
    "            # action là numpy matrix [N, M]\n",
    "            for i in range(num_honeypots_N):\n",
    "                node_idx = np.argmax(action[i])\n",
    "                node_name = env.honeypot_nodes[node_idx]\n",
    "                placement.append(f\"Honeypot {i} -> {node_name}\\n\")\n",
    "                \n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}, DSP: {dsp/interval_check*100:.3f}%\\n\")\n",
    "            print(\"\".join(placement))\n",
    "            print(path)\n",
    "            total_reward = 0\n",
    "\n",
    "            if dsp > best_dsp:\n",
    "                best_dsp = dsp\n",
    "                best_episode = episode\n",
    "                best_checkpoint = {\n",
    "                    'policy_net_state_dict': deepcopy(policy_net.state_dict()),\n",
    "                    'target_net_state_dict': deepcopy(target_net.state_dict()),\n",
    "                    'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
    "                }\n",
    "            dsp = 0\n",
    "\n",
    "        # --- Save model ---\n",
    "        if (episode + 1) % interval_save == 0 and best_checkpoint is not None:\n",
    "            os.makedirs('./Saved_Model', exist_ok=True)\n",
    "            path = f'./Saved_Model/dqn_model.pth'\n",
    "            torch.save({\n",
    "                'policy_net_state_dict': best_checkpoint['policy_net_state_dict'],\n",
    "                'target_net_state_dict': best_checkpoint['target_net_state_dict'],\n",
    "                'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],\n",
    "                'episode': best_episode},\n",
    "                path)\n",
    "            print(f'Saved model with best DSP {best_dsp} at episode {best_episode} to {path}')\n",
    "            best_dsp = 0\n",
    "            best_episode = 0\n",
    "            best_checkpoint = None\n",
    "\n",
    "    return policy_net\n",
    "\n",
    "print(\"Đã định nghĩa logic DQN (PHIÊN BẢN MULTI-HEAD, CUDA-ready).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ======================================================================\n",
    "# 5. KHỞI TẠO MÔI TRƯỜNG & HUẤN LUYỆN (PHẦN ĐÃ SỬA)\n",
    "# ======================================================================\n",
    "print(\"\\n--- Khởi tạo Môi trường RL ---\")\n",
    "\n",
    "# Initialize environment and train\n",
    "algo = global_weighted_random_attack\n",
    "G_new_env = deepcopy(G_original)\n",
    "\n",
    "# --- SỬA LỖI LOGIC QUAN TRỌNG ---\n",
    "# Chúng ta phải xử lý (pad + norm) các đặc trưng GỐC\n",
    "# để chúng khớp với đầu vào 50-dim mà encoder mong đợi.\n",
    "print(\"Xử lý đặc trưng gốc sang dạng 50-dim (Padding + Norm)...\")\n",
    "nfeats_processed_for_env = build_batch_tensor(original_node_features, MAX_N_FEATURES)\n",
    "efeats_processed_for_env = build_batch_tensor(original_edge_features, MAX_E_FEATURES)\n",
    "\n",
    "nfeats_processed_for_env = nfeats_processed_for_env.to(device)\n",
    "efeats_processed_for_env = efeats_processed_for_env.to(device)\n",
    "print(f\"Đã xử lý đặc trưng node: {nfeats_processed_for_env.shape}\")\n",
    "print(f\"Đã xử lý đặc trưng cạnh: {efeats_processed_for_env.shape}\")\n",
    "target_priority = 2 \n",
    "\n",
    "# 2. Dùng list comprehension để lọc các node từ G_new_env\n",
    "#    Chúng ta lặp qua (node, data) trong G.nodes(data=True)\n",
    "#    và chỉ giữ lại 'node' nếu 'priority' có trong 'data' VÀ data['priority'] == 2\n",
    "goal_nodes = [\n",
    "    node for node, data in G_new_env.nodes(data=True) \n",
    "    if 'priority' in data and data['priority'] == target_priority\n",
    "]\n",
    "env = NetworkEnv(\n",
    "    G_new=G_new_env,\n",
    "    attack_fn=algo,\n",
    "    g_dgl=g1, # Sử dụng g1 (DGL graph gốc)\n",
    "    encoder=trained_encoder, # Encoder đã huấn luyện (mong đợi 50-dim)\n",
    "    \n",
    "    # --- SỬA LỖI: Truyền vào các đặc trưng ĐÃ XỬ LÝ (50-dim) ---\n",
    "    original_node_features=nfeats_processed_for_env,\n",
    "    original_edge_features=efeats_processed_for_env,\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    node_map=node_map,\n",
    "    goal=goal_nodes,\n",
    "    num_honeypots = 3\n",
    ")\n",
    "\n",
    "# --- 3. HUẤN LUYỆN ---\n",
    "num_episode = 4000\n",
    "if not os.path.exists('./Saved_Model'):\n",
    "    os.makedirs('./Saved_Model')\n",
    "\n",
    "model = train_dqn(env, num_episode)"
   ],
   "id": "b21a6f640f500093",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "evaluate_model_cell",
   "metadata": {},
   "source": "evaluate_model(model, env, 2000, device)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "visualize_graph_cell",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(30, 36))\n",
    "pos = nx.spring_layout(G_original) # Dùng G_original\n",
    "\n",
    "nx.draw_networkx_nodes(G_original, pos, node_color='orange', node_size=2000)\n",
    "nx.draw_networkx_labels(G_original, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    G_original, pos,\n",
    "    edge_color='gray',\n",
    "    arrows=True,\n",
    "    arrowstyle='->',\n",
    "    arrowsize=50,\n",
    "    connectionstyle='arc3,rad=0.2'\n",
    ")\n",
    "\n",
    "# Vẽ nhãn trên cạnh\n",
    "edge_labels = {(u, v): f\"user={d['user']}, root={d['root']}\" for u, v, d in G_original.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G_original, pos, edge_labels=edge_labels, font_size=12)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc9baac7f5254d3a",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
