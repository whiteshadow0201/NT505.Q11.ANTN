{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from queue import PriorityQueue\n",
    "from utils import *\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "a60ffc5d033479e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_dqn(env, num_episodes, batch_size=10, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "    global best_checkpoint, best_episode\n",
    "    state_size = env.num_nodes\n",
    "    action_space_size = env.get_action_space_size()\n",
    "    print('state_size', state_size)\n",
    "    print('action_space_size', action_space_size)\n",
    "    # Initialize DQN and target network\n",
    "    policy_net = DQN(state_size, action_space_size)\n",
    "    target_net = DQN(state_size, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "    epsilon = epsilon_start\n",
    "    total_reward = 0\n",
    "    dsp = 0\n",
    "    best_dsp = 0\n",
    "    interval_check = num_episodes // 10  # Mỗi num_episodes/10\n",
    "    interval_save = num_episodes // 5   # Lưu sau mỗi num_episodes/5\n",
    "\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        exploration_counter = defaultdict(int)\n",
    "\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                # Chọn ngẫu nhiên index hợp lệ\n",
    "                action_idx = sample_valid_index(action_space_size, env.num_honeypot_nodes, exploration_counter)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                    q_values = policy_net(state_tensor).squeeze(0)  # shape: [action_space_size]\n",
    "\n",
    "                    # Lọc q_values chỉ lấy index hợp lệ\n",
    "                    valid_indices = [idx for idx in range(action_space_size) if is_valid_index(idx, env.num_honeypot_nodes)]\n",
    "                    valid_q_values = q_values[valid_indices]\n",
    "                    # Lấy chỉ số trong valid_indices có q_value max\n",
    "                    max_idx_in_valid = torch.argmax(valid_q_values).item()\n",
    "                    # Map về action_idx thực\n",
    "                    action_idx = valid_indices[max_idx_in_valid]\n",
    "\n",
    "            action = index_to_action(action_idx, env.num_honeypot_nodes)\n",
    "            next_state, reward, done, path, captured = env.step(action)\n",
    "            action_idx = action_to_index(action, env.num_honeypot_nodes)\n",
    "\n",
    "            # Store experience\n",
    "            replay_buffer.push(state, action_idx, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if reward == 1:\n",
    "                dsp += 1\n",
    "            # Train if enough experiences\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                states = torch.FloatTensor(states)\n",
    "                actions = torch.LongTensor(actions)\n",
    "                rewards = torch.FloatTensor(rewards)\n",
    "                next_states = torch.FloatTensor(next_states)\n",
    "                dones = torch.FloatTensor(dones)\n",
    "\n",
    "                # Compute Q-values\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0]\n",
    "                    targets = rewards + (1 - dones) * gamma * next_q_values\n",
    "\n",
    "                # Compute loss\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # Logging\n",
    "        if episode % interval_check == 0:\n",
    "            placement = []\n",
    "            for i in range(2):  # Two honeypots\n",
    "                node_idx = np.argmax(action[i])\n",
    "                node_name = env.honeypot_nodes[node_idx]\n",
    "                placement.append(f\"Honeypot {i} -> {node_name}\\n\")\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}, Defense Success Probability: {dsp/interval_check}%\\n\")\n",
    "            print(\"\".join(placement))\n",
    "            print(path)\n",
    "            total_reward = 0\n",
    "\n",
    "            # Log ra DSP lớn nhất sau mỗi num_episodes/10 iterations\n",
    "            if dsp > best_dsp:\n",
    "                best_dsp = dsp\n",
    "                best_episode = episode\n",
    "                best_checkpoint = {\n",
    "                    'policy_net_state_dict': deepcopy(policy_net.state_dict()),\n",
    "                    'target_net_state_dict': deepcopy(target_net.state_dict()),\n",
    "                    'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
    "                }\n",
    "            # Reset DSP\n",
    "            dsp = 0\n",
    "\n",
    "\n",
    "        # Save ra DSP lớn nhất sau mỗi num_episodes/5 iterations\n",
    "        if (episode + 1) % interval_save == 0 and best_checkpoint is not None:\n",
    "            path = f'./Saved_Model/dqn_model.pth'\n",
    "            torch.save({\n",
    "                'policy_net_state_dict': best_checkpoint['policy_net_state_dict'],\n",
    "                'target_net_state_dict': best_checkpoint['target_net_state_dict'],\n",
    "                'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],\n",
    "                'episode': best_episode},\n",
    "                path)\n",
    "            print(f'Saved model with best DSP {best_dsp} at episode {best_episode} to {path}')\n",
    "\n",
    "            best_dsp = 0\n",
    "            best_episode = 0\n",
    "            best_checkpoint = None\n",
    "\n",
    "    return policy_net"
   ],
   "id": "5377d66c524211df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(30, 36))\n",
    "pos = nx.spring_layout(G_original)\n",
    "\n",
    "nx.draw_networkx_nodes(G_original, pos, node_color='orange', node_size=2000)\n",
    "nx.draw_networkx_labels(G_original, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    G_original, pos,\n",
    "    edge_color='gray',\n",
    "    arrows=True,\n",
    "    arrowstyle='->',\n",
    "    arrowsize=50,\n",
    "    connectionstyle='arc3,rad=0.2'\n",
    ")\n",
    "\n",
    "# Vẽ nhãn trên cạnh\n",
    "edge_labels = {(u, v): f\"user={d['user']}, root={d['root']}\" for u, v, d in G_original.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G_original, pos, edge_labels=edge_labels, font_size=12)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "4b79226cb400a676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize environment and train\n",
    "algo = global_weighted_random_attack\n",
    "# algo = greedy_attack_priority_queue\n",
    "env = NetworkSecurityEnv(G_original, algo)\n",
    "num_episode = 10000\n",
    "model = train_dqn(env, num_episode)"
   ],
   "id": "3b20203b7880a2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_model(model,env)",
   "id": "597785563e2df568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e3762ff2a2a859dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "83e2e608a11e0e6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
