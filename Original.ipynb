{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.attack_algo_utils import *\n",
    "from utils.graph_utils import *\n",
    "from utils.utils import *"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- CHỌN PHIÊN BẢN THÍ NGHIỆM ---\n",
    "experiment_id = 1\n",
    "BASE_PATH = f'graphs/{experiment_id}'"
   ],
   "id": "f764200a08e5f484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- LOAD ENVIRONMENTAL INFORMATION ---\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Tải Môi trường (Tĩnh) ---\n",
    "STATIC_FILE_PATH = f\"{BASE_PATH}/graph_environment.pth\"\n",
    "\n",
    "try:\n",
    "    env_data = torch.load(STATIC_FILE_PATH, weights_only=False)\n",
    "\n",
    "    G = env_data['G_original']\n",
    "    node_order = env_data['node_order']\n",
    "    node_map = env_data['node_map']\n",
    "\n",
    "    # Bạn cũng có thể lấy features gốc nếu cần\n",
    "    # node_features_goc = env_data['node_features_original']\n",
    "\n",
    "    print(f\"--- Đã tải môi trường tĩnh từ '{STATIC_FILE_PATH}' ---\")\n",
    "    print(\"Tổng số node:\", len(node_order))\n",
    "    print(\"Map của 'Host 1':\", node_map['Host 1'])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"LỖI: Không tìm thấy tệp '{STATIC_FILE_PATH}'.\")\n",
    "    print(\"Vui lòng kiểm tra lại experiment_id hoặc đường dẫn.\")\n",
    "    # Thoát hoặc xử lý lỗi nếu cần\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 2. Tải Embeddings (Động) ---\n",
    "NODE_EMB_PATH = f\"{BASE_PATH}/node_embeddings.npy\"\n",
    "EDGE_EMB_PATH = f\"{BASE_PATH}/edge_embeddings.npy\"\n",
    "\n",
    "try:\n",
    "    nodes_emb = np.load(NODE_EMB_PATH)\n",
    "    edges_emb = np.load(EDGE_EMB_PATH)\n",
    "\n",
    "    print(f\"\\n--- Đã tải embedding động từ thí nghiệm {experiment_id} ---\")\n",
    "    print(\"Shape của Node Embeddings:\", nodes_emb.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"LỖI: Không tìm thấy tệp '{NODE_EMB_PATH}' hoặc '{EDGE_EMB_PATH}'.\")\n",
    "    # Thoát hoặc xử lý lỗi nếu cần\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 3. Sử dụng ---\n",
    "# Giờ đây bạn đã có cả hai:\n",
    "# - `node_map` để biết \"Host 1\" là ID số mấy.\n",
    "# - `nodes_emb` để lấy embedding của ID đó.\n",
    "\n",
    "try:\n",
    "    node_name = \"Host 1\"\n",
    "    node_id = node_map[node_name]\n",
    "    embedding_cua_host_1 = nodes_emb[node_id]\n",
    "\n",
    "    print(f\"\\n--- Sẵn sàng cho RL ---\")\n",
    "    print(f\"Embedding cho '{node_name}' (ID: {node_id}): \\n\", embedding_cua_host_1)\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"Lỗi: Không tìm thấy node '{node_name}' trong node_map.\")\n",
    "except IndexError:\n",
    "    print(f\"Lỗi: node_id {node_id} nằm ngoài phạm vi của 'nodes_emb' (Shape: {nodes_emb.shape})\")"
   ],
   "id": "8626fc0ee2bc265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Visualize the graph ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10)\n",
    "edge_labels = nx.get_edge_attributes(G, 'user')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "plt.title(\"Attack Graph (Edge Weights: User Access Prob)\")\n",
    "plt.show()"
   ],
   "id": "951c0a778822db3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(nodes_emb)",
   "id": "a60ffc5d033479e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(edges_emb)",
   "id": "1021c57f0d9d6566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ======================================================================\n",
    "# TẢI DỮ LIỆU MÔI TRƯỜNG TĨNH\n",
    "# ======================================================================\n",
    "print(\"--- Đang tải dữ liệu môi trường (tĩnh) ---\")\n",
    "env_data = torch.load(f\"{BASE_PATH}/graph_environment.pth\",weights_only=False)\n",
    "\n",
    "# Giả sử bạn đã lưu 'g_dgl' trong file .pth\n",
    "# Nếu không, bạn cần tải 'G_original' và tạo lại g_dgl\n",
    "g_dgl = env_data.get('g_dgl')\n",
    "if g_dgl is None:\n",
    "    # Nếu bạn chỉ lưu G_original, hãy tạo lại g_dgl\n",
    "    G_original = env_data['G_original']\n",
    "    g_dgl = dgl.from_networkx(G_original, node_attrs=['state', 'priority'], edge_attrs=['user', 'root'])\n",
    "    # Gán lại features gốc (rất quan trọng)\n",
    "    g_dgl.ndata['h'] = env_data['node_features_original']\n",
    "    g_dgl.edata['h'] = env_data['edge_features_original']\n",
    "    print(\"Đã tạo lại g_dgl từ G_original.\")\n",
    "\n",
    "original_edge_features = env_data['edge_features_original']\n",
    "original_node_features = env_data['node_features_original']\n",
    "static_priority_features = original_node_features[:, 1].unsqueeze(1) # Cột priority\n",
    "\n",
    "print(\"[THÀNH CÔNG] Đã tải xong dữ liệu môi trường.\")\n",
    "\n",
    "# ======================================================================\n",
    "# TẢI MODEL GNN ĐÃ HUẤN LUYỆN (Code của bạn ở đây)\n",
    "# ======================================================================\n",
    "print(\"--- Đang tải cấu hình và trọng số GNN ---\")\n",
    "\n",
    "MODEL_STATE_PATH = f\"{BASE_PATH}/dgi_model_state_dict.pth\"\n",
    "CONFIG_FILE_PATH = f\"{BASE_PATH}/model_config.pth\"\n",
    "\n",
    "try:\n",
    "    # --- 3.1: Tải file cấu hình ---\n",
    "    config = torch.load(CONFIG_FILE_PATH, weights_only=False)\n",
    "    print(f\"Đã tải cấu hình: {config}\")\n",
    "\n",
    "    # --- 3.2: Khởi tạo mô hình rỗng TỪ CẤU HÌNH ĐÃ TẢI ---\n",
    "    encoder = EGraphSAGE(\n",
    "        config['NDIM_IN'],\n",
    "        config['EDIM'],\n",
    "        config['N_HIDDEN'],\n",
    "        config['N_OUT'],\n",
    "        config['N_LAYERS'],\n",
    "        F.leaky_relu\n",
    "    )\n",
    "\n",
    "    dgi_model_to_load = DGI(encoder)\n",
    "\n",
    "    # --- 3.3: Tải trọng số đã lưu ---\n",
    "    dgi_model_to_load.load_state_dict(torch.load(MODEL_STATE_PATH, weights_only=False))\n",
    "\n",
    "    # --- 3.4: Trích xuất encoder bạn cần ---\n",
    "    trained_encoder = dgi_model_to_load.encoder\n",
    "    trained_encoder.eval() # Chuyển sang chế độ dự đoán\n",
    "\n",
    "    print(f\"[THÀNH CÔNG] Đã tải và trích xuất GNN encoder.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[LỖI] Có lỗi xảy ra khi tải model: {e}\")\n",
    "    trained_encoder = None"
   ],
   "id": "cdd8d9eedf83bf44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def train_dqn(env, num_episodes, batch_size=10, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "#     global best_checkpoint, best_episode\n",
    "#     state_size = env.num_nodes\n",
    "#     action_space_size = env.get_action_space_size()\n",
    "#     print('state_size', state_size)\n",
    "#     print('action_space_size', action_space_size)\n",
    "#     # Initialize DQN and target network\n",
    "#     policy_net = DQN(state_size, action_space_size)\n",
    "#     target_net = DQN(state_size, action_space_size)\n",
    "#     target_net.load_state_dict(policy_net.state_dict())\n",
    "#     target_net.eval()\n",
    "#\n",
    "#     optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "#     replay_buffer = ReplayBuffer(capacity=10000)\n",
    "#     epsilon = epsilon_start\n",
    "#     total_reward = 0\n",
    "#     dsp = 0\n",
    "#     best_dsp = 0\n",
    "#     interval_check = num_episodes // 10  # Mỗi num_episodes/10\n",
    "#     interval_save = num_episodes // 5   # Lưu sau mỗi num_episodes/5\n",
    "#\n",
    "#     for episode in range(1, num_episodes+1):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#\n",
    "#         exploration_counter = defaultdict(int)\n",
    "#\n",
    "#         while not done:\n",
    "#             if random.random() < epsilon:\n",
    "#                 # Chọn ngẫu nhiên index hợp lệ\n",
    "#                 action_idx = sample_valid_index(action_space_size, env.num_honeypot_nodes, exploration_counter)\n",
    "#             else:\n",
    "#                 with torch.no_grad():\n",
    "#                     state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "#                     q_values = policy_net(state_tensor).squeeze(0)  # shape: [action_space_size]\n",
    "#\n",
    "#                     # Lọc q_values chỉ lấy index hợp lệ\n",
    "#                     valid_indices = [idx for idx in range(action_space_size) if is_valid_index(idx, env.num_honeypot_nodes)]\n",
    "#                     valid_q_values = q_values[valid_indices]\n",
    "#                     # Lấy chỉ số trong valid_indices có q_value max\n",
    "#                     max_idx_in_valid = torch.argmax(valid_q_values).item()\n",
    "#                     # Map về action_idx thực\n",
    "#                     action_idx = valid_indices[max_idx_in_valid]\n",
    "#\n",
    "#             action = index_to_action(action_idx, env.num_honeypot_nodes)\n",
    "#             next_state, reward, done, path, captured = env.step(action)\n",
    "#             action_idx = action_to_index(action, env.num_honeypot_nodes)\n",
    "#\n",
    "#             # Store experience\n",
    "#             replay_buffer.push(state, action_idx, reward, next_state, done)\n",
    "#             state = next_state\n",
    "#             total_reward += reward\n",
    "#             if reward == 1:\n",
    "#                 dsp += 1\n",
    "#             # Train if enough experiences\n",
    "#             if len(replay_buffer) >= batch_size:\n",
    "#                 states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#\n",
    "#                 states = torch.FloatTensor(states)\n",
    "#                 actions = torch.LongTensor(actions)\n",
    "#                 rewards = torch.FloatTensor(rewards)\n",
    "#                 next_states = torch.FloatTensor(next_states)\n",
    "#                 dones = torch.FloatTensor(dones)\n",
    "#\n",
    "#                 # Compute Q-values\n",
    "#                 q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "#\n",
    "#                 # Compute target Q-values\n",
    "#                 with torch.no_grad():\n",
    "#                     next_q_values = target_net(next_states).max(1)[0]\n",
    "#                     targets = rewards + (1 - dones) * gamma * next_q_values\n",
    "#\n",
    "#                 # Compute loss\n",
    "#                 loss = nn.MSELoss()(q_values, targets)\n",
    "#\n",
    "#                 # Optimize\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#\n",
    "#         # Update target network\n",
    "#         if episode % 10 == 0:\n",
    "#             target_net.load_state_dict(policy_net.state_dict())\n",
    "#\n",
    "#         # Decay epsilon\n",
    "#         epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "#\n",
    "#         # Logging\n",
    "#         if episode % interval_check == 0:\n",
    "#             placement = []\n",
    "#             for i in range(2):  # Two honeypots\n",
    "#                 node_idx = np.argmax(action[i])\n",
    "#                 node_name = env.honeypot_nodes[node_idx]\n",
    "#                 placement.append(f\"Honeypot {i} -> {node_name}\\n\")\n",
    "#             print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}, Defense Success Probability: {dsp/interval_check}%\\n\")\n",
    "#             print(\"\".join(placement))\n",
    "#             print(path)\n",
    "#             total_reward = 0\n",
    "#\n",
    "#             # Log ra DSP lớn nhất sau mỗi num_episodes/10 iterations\n",
    "#             if dsp > best_dsp:\n",
    "#                 best_dsp = dsp\n",
    "#                 best_episode = episode\n",
    "#                 best_checkpoint = {\n",
    "#                     'policy_net_state_dict': deepcopy(policy_net.state_dict()),\n",
    "#                     'target_net_state_dict': deepcopy(target_net.state_dict()),\n",
    "#                     'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
    "#                 }\n",
    "#             # Reset DSP\n",
    "#             dsp = 0\n",
    "#\n",
    "#\n",
    "#         # Save ra DSP lớn nhất sau mỗi num_episodes/5 iterations\n",
    "#         if (episode + 1) % interval_save == 0 and best_checkpoint is not None:\n",
    "#             path = f'./Saved_Model/dqn_model.pth'\n",
    "#             torch.save({\n",
    "#                 'policy_net_state_dict': best_checkpoint['policy_net_state_dict'],\n",
    "#                 'target_net_state_dict': best_checkpoint['target_net_state_dict'],\n",
    "#                 'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],\n",
    "#                 'episode': best_episode},\n",
    "#                 path)\n",
    "#             print(f'Saved model with best DSP {best_dsp} at episode {best_episode} to {path}')\n",
    "#\n",
    "#             best_dsp = 0\n",
    "#             best_episode = 0\n",
    "#             best_checkpoint = None\n",
    "#\n",
    "#     return policy_net"
   ],
   "id": "5377d66c524211df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_dqn(env, num_episodes, batch_size=10, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "    global best_checkpoint, best_episode\n",
    "    # 1. Reset env để lấy state (embedding) ban đầu\n",
    "    state = env.reset() # state giờ là Tensor [num_nodes, embedding_dim]\n",
    "\n",
    "    # 2. Tính toán state_size đã làm phẳng\n",
    "    num_nodes = state.shape[0]\n",
    "    embedding_dim = state.shape[1]\n",
    "    state_size = num_nodes * embedding_dim  # <--- Kích thước input mới cho DQN\n",
    "\n",
    "    action_space_size = env.get_action_space_size()\n",
    "\n",
    "    print('state_size (flattened):', state_size) # <--- Cập nhật log\n",
    "    print('action_space_size', action_space_size)\n",
    "\n",
    "    # 3. Khởi tạo DQN với state_size mới\n",
    "    policy_net = DQN(state_size, action_space_size)\n",
    "    target_net = DQN(state_size, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "    epsilon = epsilon_start\n",
    "    total_reward = 0\n",
    "    dsp = 0\n",
    "    best_dsp = 0\n",
    "    interval_check = num_episodes // 10\n",
    "    interval_save = num_episodes // 5\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Reset state cho các episode sau\n",
    "        if episode > 1:\n",
    "            state = env.reset() # <--- state là Tensor embedding\n",
    "\n",
    "        done = False\n",
    "        exploration_counter = defaultdict(int)\n",
    "\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action_idx = sample_valid_index(action_space_size, env.num_honeypot_nodes, exploration_counter)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    # --- THAY ĐỔI: Flatten state tensor ---\n",
    "                    # Chuyển [N, D] -> [1, N*D]\n",
    "                    state_tensor = state.flatten().unsqueeze(0)\n",
    "                    q_values = policy_net(state_tensor).squeeze(0)\n",
    "\n",
    "                    # (Logic lọc q_values giữ nguyên)\n",
    "                    valid_indices = [idx for idx in range(action_space_size) if is_valid_index(idx, env.num_honeypot_nodes)]\n",
    "                    valid_q_values = q_values[valid_indices]\n",
    "                    max_idx_in_valid = torch.argmax(valid_q_values).item()\n",
    "                    action_idx = valid_indices[max_idx_in_valid]\n",
    "\n",
    "            action = index_to_action(action_idx, env.num_honeypot_nodes)\n",
    "\n",
    "            # --- next_state giờ cũng là Tensor embedding ---\n",
    "            next_state, reward, done, path, captured = env.step(action)\n",
    "            action_idx = action_to_index(action, env.num_honeypot_nodes)\n",
    "\n",
    "            # Store experience (state và next_state là Tensors)\n",
    "            replay_buffer.push(state, action_idx, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if reward == 1:\n",
    "                dsp += 1\n",
    "\n",
    "            # Train if enough experiences\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # --- THAY ĐỔI: Replay buffer giờ trả về Tensors ---\n",
    "                states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # states_batch là [B, N, D], actions_batch là [B], rewards_batch là [B, 1], ...\n",
    "\n",
    "                # --- THAY ĐỔI: Flatten state batches ---\n",
    "                # Chuyển [B, N, D] -> [B, N*D]\n",
    "                states_flat = states_batch.flatten(start_dim=1)\n",
    "                next_states_flat = next_states_batch.flatten(start_dim=1)\n",
    "\n",
    "                # Compute Q-values\n",
    "                q_values_all = policy_net(states_flat)\n",
    "                # Dùng actions_batch để lấy Q-value của action đã chọn\n",
    "                q_values = q_values_all.gather(1, actions_batch.long().unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    # Dùng next_states_flat\n",
    "                    next_q_values = target_net(next_states_flat).max(1)[0]\n",
    "                    # Squeeze rewards và dones để khớp kích thước [B]\n",
    "                    targets = rewards_batch.squeeze(1) + (1 - dones_batch.squeeze(1)) * gamma * next_q_values\n",
    "\n",
    "                # Compute loss\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # Logging (Giữ nguyên)\n",
    "        if episode % interval_check == 0:\n",
    "            placement = []\n",
    "            for i in range(2):\n",
    "                node_idx = np.argmax(action[i])\n",
    "                node_name = env.honeypot_nodes[node_idx]\n",
    "                placement.append(f\"Honeypot {i} -> {node_name}\\n\")\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}, Defense Success Probability: {dsp/interval_check}%\\n\")\n",
    "            print(\"\".join(placement))\n",
    "            print(path)\n",
    "            total_reward = 0\n",
    "\n",
    "            if dsp > best_dsp:\n",
    "                best_dsp = dsp\n",
    "                best_episode = episode\n",
    "                best_checkpoint = {\n",
    "                    'policy_net_state_dict': deepcopy(policy_net.state_dict()),\n",
    "                    'target_net_state_dict': deepcopy(target_net.state_dict()),\n",
    "                    'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
    "                }\n",
    "            dsp = 0\n",
    "\n",
    "        # Save (Giữ nguyên)\n",
    "        if (episode + 1) % interval_save == 0 and best_checkpoint is not None:\n",
    "            path = f'./Saved_Model/dqn_model.pth'\n",
    "            torch.save({\n",
    "                'policy_net_state_dict': best_checkpoint['policy_net_state_dict'],\n",
    "                'target_net_state_dict': best_checkpoint['target_net_state_dict'],\n",
    "                'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],\n",
    "                'episode': best_episode},\n",
    "                path)\n",
    "            print(f'Saved model with best DSP {best_dsp} at episode {best_episode} to {path}')\n",
    "\n",
    "            best_dsp = 0\n",
    "            best_episode = 0\n",
    "            best_checkpoint = None\n",
    "\n",
    "    return policy_net"
   ],
   "id": "1c293e1d3a7ce59a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(30, 36))\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color='orange', node_size=2000)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    G, pos,\n",
    "    edge_color='gray',\n",
    "    arrows=True,\n",
    "    arrowstyle='->',\n",
    "    arrowsize=50,\n",
    "    connectionstyle='arc3,rad=0.2'\n",
    ")\n",
    "\n",
    "# Vẽ nhãn trên cạnh\n",
    "edge_labels = {(u, v): f\"user={d['user']}, root={d['root']}\" for u, v, d in G.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "4b79226cb400a676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize environment and train\n",
    "algo = global_weighted_random_attack\n",
    "# algo = greedy_attack_priority_queue\n",
    "# Tạo một bản sao của đồ thị cho môi trường\n",
    "G_new_env = deepcopy(G)\n",
    "\n",
    "env = NetworkEnv(\n",
    "    G_new=G_new_env,\n",
    "    attack_fn=algo,\n",
    "    g_dgl=g_dgl,\n",
    "    encoder=encoder,\n",
    "    original_node_features=original_node_features,\n",
    "    original_edge_features=original_edge_features,\n",
    "    node_map=node_map,\n",
    "    goal=\"Data Server\"  # (Hoặc goal bạn muốn)\n",
    ")\n",
    "\n",
    "# --- 3. HUẤN LUYỆN (Như cũ) ---\n",
    "num_episode = 10000\n",
    "!mkdir Saved_Model\n",
    "model = train_dqn(env, num_episode)"
   ],
   "id": "3b20203b7880a2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_model(model,env)",
   "id": "597785563e2df568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "97952a87816fd266",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
